---
layout: post
category: python
title: 第68天： Python Scrapy 项目实战
tagline: by 戴景波
tags: 
  - python100
---

# 爬虫编写流程

首先明确 Python 爬虫代码编写的流程：先直接打开网页，找到你想要的数据，就是走一遍流程。比如这个项目我要爬取历史某一天所有比赛的赔率数据、每场比赛的比赛结果等。

那么我就先打开这个网址：https://live.leisu.com/wanchang?date=20190606 然后点击“竞彩”，再点击“指数”，跳转到另一个网址：https://live.leisu.com/3in1-2674547，然后就看到了想要的数据：各公司主队获胜赔率1.61、1.65等。

到此为止，开始动手通过代码实现这个过程。

<!--more-->

# 解析“爬虫主程序.py” ：（主程序包括四个函数）

## start_requests 

向 https://live.leisu.com/wanchang?date=20190606 发送请求。（你可以打开这个网址，里边是爬虫程序爬取数据的最外层网站）
scrapy.http.FormRequest 方法：第一个参数是请求的具体网址；第二个参数是下一步调用的函数；第三个参数 meta 是向调用函数传递的参数。

## parseLs （parseWl 同理，不再重复讲解）

主要用于解析次外层网页数据。这里用 XPath 解析，也是比较容易掌握的解析方式。网页结构如下：（通过 Google 浏览器打开https://live.leisu.com/wanchang?date=20190606 然后右键点击网页空白处点击“查看网页源代码”，找到你需要爬取的核心数据部分，这里我要找每场比赛的信息，那么拷贝下来，然后以易于查看的规整方式列出，如下：）

```xml
<li class="list-item list-item-2674547 list-day-6-6 finished " data-id="2674547" data-status="8" data-eventid="2906" data-status-name="finished" data-nowtime="1559760300" data-realtime="1559764089" data-eventlevels="1" data-halftime="45,15" data-lottery="周三001,北单018," data-asian-name="name-0.25" data-daxiao-name="name-2.5" data-asian="1.125,0.25,0.78,0" data-daxiao="0.99,2.5,0.91,0" data-home-icon="8863b9e186e3580aa6dec29f19155d3a.png" data-away-icon="f84be480c54f0ff871b91fab14a36b36.png" style="height:41px;">
<div class="find-table layout-grid-tbody hide">
<div class="clearfix-row">
<i class="live-level live-1"></i> 
<span class="lab-events"> 
<span class="event-icon" style="background-image:url(//cdn.leisu.com/eventlogo/245bff452fbdc34d417164e361097ae7.png?imageView2/2/w/40);">
</span> 
<a class="event-name" href="//data.leisu.com/zuqiu-8571/" target="_blank"><span class="display-i-b w-bar-100 line-h-14 v-a-m lang " data-lang="欧国联,歐國聯,UEFA  NL"> 欧国联 </span>
</a>
</span> 
<span class="lab-round"> 0</span> 
<span class="lab-lottery"> 
<span class="text-jc">周三001</span> 
<span class="text-bd">北单018</span> 
<span class="text-zc"></span>
</span> 
......

```

parseLS函数里的下边代码，结合上表中 xml 中的元素：获取了比赛场次，存储到item['cc']。

```python
sel_div=sel('//li[@data-id='+str(raceid[0])+']/div[@class="find-table layout-grid-tbody hide"]/div[@class="clearfix-row"]')
if str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()) == "[]":
    item['cc']=""
else:
    item['cc']=str(d2) + str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()[0])
```

此外，还要获取比赛的赔率信息，但并不在当前这个网页，而在更内层的网页中，需要从当前网页跳转。
存储赔率的内层网页为 https://live.leisu.com/3in1-2674547，不同场次的比赛只有-后边的数字是变化的，那么程序中只要循环构造对应的数字2674547就好了。发现这个数字刚好是 data-id。通过以下代码实现获取：

```python
racelist=[e5.split("'") for e5 in sel('//li[@data-status="8"]/@data-id').extract()]
for raceid in racelist:
    plurl='https://live.leisu.com/3in1-'+raceid[0]
    request = scrapy.http.FormRequest(plurl,callback=self.parse,meta={'item':item})
    yield request
```
再提交该网页请求到下一个函数parse。

## parse

网页结构如下：（通过Google浏览器打开https://live.leisu.com/3in1-2674547 然后右键点击网页空白处点击“查看网页源代码”，拷贝需要赔率的部分到文本文档，换行操作后如下：
```xml
<tr class="td-data td-pd-8 f-s-12 color-666 bd-top " data-id="4">
<td> 
<label class="lab-check" onclick="threeInOne.check(this);">
<i class="iconfont choicen m-r-5">
</i>
</label>
</td>
<td class="w-150 bd-left" data-live="0,0"> 
<span class="w-20 h-20 m-l-16 line-h-19 b-radius-3 color-fff f-s-12 float-left">&nbsp;
</span> 
<span class="line-h-22 float-left m-l-5 w-105 word-break text-a-l"> 10BET 
</span>
</td>
<td class="bd-left">
<div class="begin float-left w-bar-100 bd-bottom p-b-8 color-999 m-b-8">
<span class="float-left col-3"> 1.620 </span>
<span class="float-left col-3"> 3.600 </span> 
<span class="float-left col-3"> 5.250 </span>
</div>
......

```
通过以下代码获取赔率：
```python
pl_str = '/td[@class="bd-left"]/div[@class="begin float-left w-bar-100 bd-bottom p-b-8 color-999 m-b-8"]/span[@class="float-left col-3"]/text()'
if str(pv('//*[@data-id="5"]'+pl_str).extract())=="[]":
     item['li'] =  ''
else:
     item['li']=pv('//*[@data-id="5"]' + pl_str).extract()[0]

```

本篇的全部源码（可执行）：(github.com.cn/acredjb/FBP有完整项目爬虫源码)

```python
# -*- coding: utf-8 -*-
#项目源码地址：github.com/acredjb/FBP
#作者：acredjb
# scrapy crawl FBP -o BaseData.csv
import datetime
import sys
import requests
import scrapy
import time
import json
import scrapy.http
from peilv.items import PeilvItem
from lxml import etree
#获取当天或未来某天数据的地址
wl_url = 'https://live.leisu.com/saicheng?date='#wl历史https://live.leisu.com/saicheng?date=20190620
#获取历史数据的地址
ls_url = 'https://live.leisu.com/wanchang?date='#ls历史https://live.leisu.com/wanchang?date=20190606
class LiveJiangSpider(scrapy.Spider):
    name = 'FBP'
    allowed_domains = ['leisu.com']
    def start_requests(self):
            d1='20190606' #历史的比赛
            # d1='20190914' #未来的比赛
            request = scrapy.http.FormRequest(ls_url + d1,callback=self.parseLs, meta={'d1': d1}) #历史的比赛
            # request = scrapy.http.FormRequest(wl_url + d1,callback=self.parseWl, meta={'d1': d1})#未来的比赛
            yield request
    def parseLs(self,response):
        d2=response.meta['d1']
        sel=response.xpath
        racelist=[e5.split("'") for e5 in sel('//li[@data-status="8"]/@data-id').extract()]
        for raceid in racelist:#raceid=['2674547'];raceid[0]=2674547
            item = PeilvItem()
            sel_div=sel('//li[@data-id='+str(raceid[0])+']/div[@class="find-table layout-grid-tbody hide"]/div[@class="clearfix-row"]')
            if str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()) == "[]":
                item['cc']=""
            else:
                item['cc']=str(d2) + str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()[0])
            if "周" in item['cc']:#取竞彩-周一001等
                plurl='https://live.leisu.com/3in1-'+raceid[0]
                request = scrapy.http.FormRequest(plurl,callback=self.parse,meta={'item':item})
                yield request #并非return，yield压队列，parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并没有直接执行parse，循环完成后，再执行parse

    def parseWl(self,response):
        d2=response.meta['d1'] #接收参数
        sel=response.xpath
        racelist=[e5.split("'") for e5 in sel('//li[@data-status="1"]/@data-id').extract()]
        for raceid in racelist:#raceid=['2674547'];raceid[0]=2674547
            item = PeilvItem()
            sel_div=sel('//*[@data-id=' + str(raceid[0]) + ']/div[@class="find-table layout-grid-tbody hide"]/div[@class="clearfix-row"]')
            if str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()) == "[]":
                changci=""
            else:
                changci =str(sel_div.xpath('span[@class="lab-lottery"]/span[@class="text-jc"]/text()').extract()[0])

            if "周" in changci:#取竞彩-周一001等
                item['cc']=str(d2) + changci
                plurl='https://live.leisu.com/3in1-'+raceid[0]
                request = scrapy.http.FormRequest(plurl, callback=self.parse,meta={'item':item})
                yield request #并非return，yield压队列，parse函数将会被当做一个生成器使用。scrapy会逐一获取parse方法中生成的结果，并没有直接执行parse，循环完成后，再执行parse
    def parse(self, response):
        print('--------------into parse----------------------')
        item = response.meta['item']
        pv=response.xpath
        pl_str = '/td[@class="bd-left"]/div[@class="begin float-left w-bar-100 bd-bottom p-b-8 color-999 m-b-8"]/span[@class="float-left col-3"]/text()'

        if str(pv('//*[@data-id="5"]'+pl_str).extract())=="[]":
            item['li'] =  ''
        else:
            item['li']=pv('//*[@data-id="5"]' + pl_str).extract()[0]

        if str(pv('//*[@data-id="2"]'+pl_str).extract())=="[]":
            item['b5'] =  ''
        else:
            item['b5']=pv('//*[@data-id="2"]' + pl_str).extract()[0]

        yield item#程序在取得各个页面的items前，会先处理完之前所有的request队列里的请求，然后再提取items

```

## 总结

以上我们实现了一个爬虫实战项目，通过分析网页结构，借助 Scrapy 框架获取数据，为今后的数据分析做准备。

## 代码地址

[python Scrapy 项目实战](https://github.com/JustDoPython/python-100-day/tree/master/day-036)


> 示例代码：[Python-100-days-day036](https://github.com/JustDoPython/python-100-day/tree/master/day-036)

