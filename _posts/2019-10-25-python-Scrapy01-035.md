---
layout: post
category: python
title: 第35天：Python Scrapy爬虫框架及搭建
tagline: by 戴景波
tags: 
  - python100
---

# Python爬虫基本流程：

## A发起请求—B解析内容—C获取响应内容—D保存数据

A通过HTTP向目标站点发起请求，即发送一个Request，请求可以包含额外的headers等信息，等待服务器响应。

B得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。

C如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，类型可能有HTML，Json字符串，二进制数据（如图片视频）等类型。

D保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。

# Scrapy框架实现爬虫的基本原理：

Scrapy就是封装好的框架，你可以专心编写爬虫的核心逻辑，无需自己编写与爬虫逻辑无关的代码，套用这个框架就可以实现以上功能——爬取到想要的数据。下图中给出了Scrapy实现爬虫的原理，如果暂时理解不深也没关系，后边会结合实例具体介绍。

搭建自己本机环境如下：Windows7 64bit  Python3.7  Pycharm64

## 安装Python安装Pycharm安装Scrapy新建爬虫项目机器学习建模

简单解释：将Python比作Java，那么Pycharm就相当于eclipse，Pycharm就是Python语言的运行环境IDE工具。

# 安装Python
在Python的官网 www.python.org 中找到最新版本的Python安装包，点击进行下载，请注意，当你的电脑是32位的机器，请选择32位的安装包，如果是64位的，请选择64位的安装包；我自己机器是win7 64bit所以我下载的是python-3.7.4.amd64.exe，其中的add python 3.7 to PATH一定要勾选。另外安装python路径不要有中文和空格，避免以后麻烦。后边就点击下一步即可。
如果忘记勾选则需要手动添加环境变量：（需要添加两个：c:\python3.7.0;c:\python3.7.0\Scripts）
右键计算机——点击属性——点击高级系统设置——高级——环境变量——用户变量——PATH  添加自己安装python的路径。

# 安装Pycharm
本篇对于环境的搭建只是起到抛砖引玉的作用，建议大家以下边做参考。https://www.runoob.com/w3cnote/pycharm-windows-install.html

# 安装Scrapy
由于安装Scrapy不是本系列重点，所以仅展示Windows系统上安装Scrapy的步骤。注意：一定按顺序安装。Cmd进入dos窗口：
C:\Users\Administrator>python
Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
C:\Users\Administrator>python -m pip -V
pip 19.0.3 from c:\python3.7.0\lib\site-packages\pip (python 3.7)  
1.python -m pip install --upgrade pip
2.python -m pip install Twisted-18.9.0-cp37-cp37m-win_amd64.whl
3.python -m pip install pypiwin32
4.python -m pip install scrapy
5.python -m pip install requests
如果中途安装遇到问题请及时Google查阅资料，查阅就是积累的过程。

# Scrapy创建新项目：
Pycharm中用alt+F12切换到命令行，在命令行输入：
(venv2) E:\>scrapy startproject peilv
就会生成Scrapy项目，项目名称是peilv，结构如下：主要改写2个文件：“items、settings”，新增2个文件：“爬虫主程序”、itemcsvexporter。
peilv
scrapy.cfg                     #创建项目时自动生成，项目的配置文件
peilv/
    __init__.py                #创建项目时自动生成，无需任何改动
    items.py                   #创建项目时自动生成，定义爬取的字段
    pipelines.py               #创建项目时自动生成，如存入文件，无需任何改动
    settings.py                #创建项目时自动生成，将爬取字段按顺序输出
    middlewares.py             #创建项目时自动生成，无需任何改动
    spiders/
        __init__.py            #创建项目时自动生成，无需任何改动
	itemcsvexporter.py     #需自己编写，代码固定
        爬虫主程序.py          #需自己编写，爬虫的主程序
	
# items.py：
```python
# -*- coding: utf-8 -*-
import scrapy
class PeilvItem(scrapy.Item):
    # define the fields for your item here like:
    cc  = scrapy.Field()#changci
    li =  scrapy.Field()#libo
    b5  = scrapy.Field()#bet365
```
# settings.py：
```python
# -*- coding: utf-8 -*-
BOT_NAME = 'peilv'
SPIDER_MODULES = ['peilv.spiders']
NEWSPIDER_MODULE = 'peilv.spiders'
FEED_EXPORT_ENCODING = "gb18030" #解决导出的Excel文件中文乱码问题
user_agent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1"
FEED_URI = 'file:///e:/PythonLearn/Python learning/peilv/BaseData.csv'
FEED_FORMAT = 'csv'
FEED_EXPORTERS = {
    'csv': 'peilv.spiders.itemcsvexporter.itemcsvexporter',
}  # 这里你的project名字为peilv
FIELDS_TO_EXPORT = [
    'cc',#比赛场次
    'li',#立博的赔率
    'b5',#bet365的赔率
   ]
ROBOTSTXT_OBEY = False #当用cookies时候要设置为false
DOWNLOADER_MIDDLEWARES = {
'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
}
HTTPERROR_ALLOWED_CODES = [403]
```
# itemcsvexporter.py：
```python
from scrapy.conf import settings
# from scrapy.contrib.exporter import CsvItemExporter
from scrapy.exporters import CsvItemExporter
#指定输出到csv文件中字段的顺序，结合setting.py
class itemcsvexporter(CsvItemExporter):
    def __init__(self, *args, **kwargs):
        delimiter = settings.get('CSV_DELIMITER', ',')
        kwargs['delimiter'] = delimiter
        fields_to_export = settings.get('FIELDS_TO_EXPORT', [])
        if fields_to_export:
            kwargs['fields_to_export'] = fields_to_export
        super(itemcsvexporter, self).__init__(*args, **kwargs)
```
# 爬虫主程序.py：(下一篇详细介绍)
```python
# -*- coding: utf-8 -*-
#项目源码地址：github.com/acredjb/FBP
#作者：acredjb
...
```
改写完程序后，最终执行命令：
Pycharm用alt+F12切换到命令行在项目peilv路径上执行：
(venv2) E:\>cd peilv\peilv 
(venv2) E:\peilv\peilv>scrapy crawl FBP -o BaseData.csv
其中FBP是在“爬虫主程序.py”定义的——name = 'FBP'，“-o BaseData.csv”是将爬取的数据输出到csv文件中。

## 总结
以上我们以一个实战项目为依托，将建立Scrapy项目的过程从零开始，深入浅出，让读者能够实践爬虫的整个过程。

## 代码地址
[python Scrapy爬虫框架及搭建](https://github.com/JustDoPython/python-100-day/tree/master/day-035)


> 示例代码：[Python-100-days-day035](https://github.com/JustDoPython/python-100-day/tree/master/day-035)

