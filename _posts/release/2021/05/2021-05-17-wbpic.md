---
layout: post
title: 还在搜百度图片？太LOW了！
category: python
tagline: by 闲欢
tags: 
  - python
  - 爬虫
  - 图片
---

你现在还去什么图片网站搜图片吗？

你现在还在百度图片等图片网站搜素材吗？

今天给大家分享一种全新的方式来获取图片素材，你想要的这里全都有！

它就是新浪微博！全新一代的超级流量入口，甚至好多人现在将其作为替代百度搜索的工具，以前是有事问度娘，现在是有事搜微博。

比如，某宅男想要看清纯少女图片，越多越好的那种。他会这样搜：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/1.jpg)

然后，他会一页一页地翻阅欣赏这样的养眼图片，遇到特别喜欢的还会下载大图到本地保存。

![](http://www.justdopython.com/assets/images/2021/05/wbpic/2.jpg)

作为一个 python 程序员，我是懒得这样一页又一页的翻的，我喜欢全部下载下来慢慢欣赏，撑爆硬盘的那种！类似下面这样：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/3.jpg)

这种简单问题对于我们会写爬虫的 pythoner 来说，简直易如反掌，何乐而不为呢？下面直接给大家分享一下。


### 分析目标网站

微博这个搜索结果是分页的，所以我们的思路肯定是按分页来处理，逐页解析。

我们先来看看搜索结果：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/4.jpg)

我们点击“搜索”按钮后，很容易就可以发现这个搜索的请求，这里网页呈现的是第一页的结果。

接着我们点击“下一页”，可以发现请求变成下面这样：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/5.jpg)

细心的你可能一眼就发现了，请求后面多了一个`page`参数，它指的是页码，这样就很容易了，我们只需要改变这个页码就可以请求到相应页面的内容。

由于这个请求返回的是一个 HTML 页面，所以我们需要在这个页面上下功夫，找到图片的链接。我们随便使用页面元素检查来定位一张图片，就会看到下面的效果：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/6.jpg)

我们可以看到这个 `div` 里面的 `action-data` 是一个合集，而下面的 `li` 里面的 `img` 的 `action-data` 是单个图片的数据。

接下来，我们点击“查看大图”按钮，浏览器会跳转到新页面，页面内容就是一张大图。

![](http://www.justdopython.com/assets/images/2021/05/wbpic/7.jpg)

从请求的 URL 中顺着找下来，我找到了这个：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/8.jpg)

我复制这个 URL 到浏览器请求，发现这正是我们需要的大图。我们也很容易看到这个 URL 的特点，就是在最后把我们上面的 `action-data` 里面的 `pic_id` 加进去了。

这样一来，我们的思路就很清晰了，下面是获取一页图片的思路：

1. 获取到页面内容；
2. 在页面内容中找到图片的ID；
3. 拼接查看大图的 URL 进行请求，获得图片；
4. 将图片写到本地。

### 码代码

思路理清楚了，需要码代码来验证。由于代码相对简单，这里就一次性呈现给大家了。

```python
import requests
import re
import os
import time
cookie = {
    'Cookie': 'your cookie'
}
header = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4144.2 Safari/537.36'
}
get_order = input('是否启动程序? yes or no:   ')
number = 1
store_path = 'your path'
while True:
    if get_order != 'no':
        print('抓取中......')  # 下面的链接填写微博搜索的链接
        url = f'https://s.weibo.com/weibo?q=%23%E5%B0%91%E5%A5%B3%E5%86%99%E7%9C%9F%23&wvr=6&b=1&Refer=SWeibo_box&page={number}'
        response = requests.get(url, cookies=cookie)
        result = response.text
        print(result)
        detail = re.findall('data="uid=(.*?)&mid=(.*?)&pic_ids=(.*?)">', result)
        for part in detail:
            uid = part[0]
            mid = part[1]
            pids = part[2]
            for picid in pids.split(','):
                url_x = f'https://wx1.sinaimg.cn/large/%s.jpg'%picid  # 这里就是大图链接了
                response_photo = requests.get(url_x, headers=header)
                file_name = url_x[-10:]
                if not os.path.exists(store_path+uid):
                    os.mkdir(store_path+uid)
                with open(store_path+uid + '/' + file_name, 'ab') as f:  # 保存文件
                    f.write(response_photo.content)
                    time.sleep(0.5)
        print('获取完毕')
        get_order = input('是否继续获取下一页? Y:yes N:no:   ')
        if get_order != 'no':
            number += 1
        else:
            print('程序结束')
            break
    else:
        print('程序结束')
        break

```

我这里使用了逐页断点的形式，目的是方便调试，大家如果想一次性获取所有页的图片，可以去掉这些输入判断，直接一次性跑完。代码里面的 cookie 和 图片存放路径需要大家替换成自己本地的。

跑完的结果如下：

![](http://www.justdopython.com/assets/images/2021/05/wbpic/9.jpg)

这里面我把图片按照微博分了文件夹，如果你觉得不直观，想把所有图片都放在一个目录下，可以去掉这一层文件夹。


 
### 总结

本篇介绍了如何通过简单爬虫来获取微博图片搜索结果，代码量相当少，但是对某些人来说却很有帮助，省时省力。大家也可以扩展一下，除了图片，也可以去获取其他的搜索内容，比如微博正文、视屏、购物链接等等。


> 示例代码：(https://github.com/JustDoPython/python-examples/tree/master/xianhuan/wbpic)